{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d357209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import allergen mapping\n",
    "import json\n",
    "with open(\"food_allergen_map.json\") as f:\n",
    "    MAP = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d45dc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "def split_df(data_dir=\"food-101\"):\n",
    "    with open(f\"{data_dir}/meta/test.txt\") as f:\n",
    "        test_list = f.read().splitlines()\n",
    "    with open(f\"{data_dir}/meta/train.txt\") as f:\n",
    "        train_list = f.read().splitlines()\n",
    "\n",
    "    for split, file_list in zip([\"train\", \"test\"], [train_list, test_list]):\n",
    "        for item in file_list:\n",
    "            label = item.split(\"/\")[0]\n",
    "            src = os.path.join(data_dir, \"images\", item + \".jpg\")\n",
    "            dst_dir = os.path.join(\"food101_split\", split, label)\n",
    "            os.makedirs(dst_dir, exist_ok=True)\n",
    "            shutil.copy(src, os.path.join(dst_dir, os.path.basename(item) + \".jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa4d52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "132cb5bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 75750\n",
      "Test images: 25250\n",
      "Train/Test Split: 0.75/0.25\n"
     ]
    }
   ],
   "source": [
    "# Check split\n",
    "train_list = open(\"food-101/meta/train.txt\").read().splitlines()\n",
    "test_list = open(\"food-101/meta/test.txt\").read().splitlines()\n",
    "\n",
    "train_len = len(train_list)\n",
    "test_len = len(test_list)\n",
    "total = train_len + test_len\n",
    "\n",
    "train_ratio = train_len / total\n",
    "test_ratio = test_len / total\n",
    "\n",
    "print(f\"Train images: {train_len}\")\n",
    "print(f\"Test images: {test_len}\")\n",
    "print(f\"Train/Test Split: {train_ratio:.2f}/{test_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5777e6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Alin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Large_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\" to C:\\Users\\Alin/.cache\\torch\\hub\\checkpoints\\mobilenet_v3_large-8738ca79.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21.1M/21.1M [00:06<00:00, 3.25MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "from torchvision.models import mobilenet_v3_large\n",
    "import torch.nn as nn\n",
    "\n",
    "model = mobilenet_v3_large(pretrained=True)\n",
    "model.classifier[3] = nn.Linear(model.classifier[3].in_features, 101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "041b4605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf52bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizing\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d4e124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                         [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"food101_split/train\", transform=transform)\n",
    "test_dataset = datasets.ImageFolder(\"food101_split/test\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "054be357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [1/10]: 100%|█████████| 2368/2368 [56:42<00:00,  1.44s/it, Batch Loss=2.1755, Avg Loss=1.7890]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 1 completed in 3402.52s - Avg Loss: 1.7890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [2/10]: 100%|█████████| 2368/2368 [51:48<00:00,  1.31s/it, Batch Loss=2.2172, Avg Loss=1.0438]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 2 completed in 3108.79s - Avg Loss: 1.0438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [3/10]: 100%|█████████| 2368/2368 [55:24<00:00,  1.40s/it, Batch Loss=0.2014, Avg Loss=0.7873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 3 completed in 3324.44s - Avg Loss: 0.7873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [4/10]: 100%|███████| 2368/2368 [1:01:43<00:00,  1.56s/it, Batch Loss=1.0794, Avg Loss=0.6001]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 4 completed in 3703.71s - Avg Loss: 0.6001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [5/10]: 100%|█████████| 2368/2368 [57:57<00:00,  1.47s/it, Batch Loss=1.2432, Avg Loss=0.4492]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 5 completed in 3477.87s - Avg Loss: 0.4492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [6/10]: 100%|█████████| 2368/2368 [55:01<00:00,  1.39s/it, Batch Loss=0.1019, Avg Loss=0.3347]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 6 completed in 3301.21s - Avg Loss: 0.3347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [7/10]: 100%|█████████| 2368/2368 [59:47<00:00,  1.51s/it, Batch Loss=0.5720, Avg Loss=0.2552]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 7 completed in 3587.17s - Avg Loss: 0.2552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [8/10]: 100%|███████| 2368/2368 [1:06:52<00:00,  1.69s/it, Batch Loss=0.2949, Avg Loss=0.2003]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 8 completed in 4012.13s - Avg Loss: 0.2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [9/10]: 100%|███████| 2368/2368 [1:06:21<00:00,  1.68s/it, Batch Loss=0.7819, Avg Loss=0.1564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 9 completed in 3981.80s - Avg Loss: 0.1564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch [10/10]: 100%|██████| 2368/2368 [1:06:37<00:00,  1.69s/it, Batch Loss=0.9088, Avg Loss=0.1336]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Epoch 10 completed in 3997.83s - Avg Loss: 0.1336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    start_time = time.time()\n",
    "\n",
    "    loop = tqdm(enumerate(train_loader), total=len(train_loader), ncols=100)\n",
    "    loop.set_description(f\"Epoch [{epoch+1}/{EPOCHS}]\")\n",
    "\n",
    "    for batch_idx, (images, labels) in loop:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # Display live loss info like YOLO\n",
    "        loop.set_postfix({\n",
    "            \"Batch Loss\": f\"{loss.item():.4f}\",\n",
    "            \"Avg Loss\": f\"{running_loss / (batch_idx + 1):.4f}\"\n",
    "        })\n",
    "\n",
    "    epoch_time = time.time() - start_time\n",
    "    print(f\"✅ Epoch {epoch+1} completed in {epoch_time:.2f}s - Avg Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5292d569",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mobilenetv3_food101.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a3173",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m total = \u001b[32m0\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mval_loader\u001b[49m:\n\u001b[32m      7\u001b[39m         images, labels = images.to(device), labels.to(device)\n\u001b[32m      8\u001b[39m         outputs = model(images)\n",
      "\u001b[31mNameError\u001b[39m: name 'val_loader' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
